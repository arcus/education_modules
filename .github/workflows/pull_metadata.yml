# This is a workflow to pull all of the metadata from all of the modules into a single file
# each time something is merged to main, this workflow will run and rebuild the metadata file.

name: Process Module Metadata


# Controls when the workflow will run
on:
  # Triggers the workflow on push or pull request events but only for the "main" branch
  push:
    branches: "atomize_metadata_workflows" #CHANGE TO MAIN BEFORE MERGING
    paths-ignore:
      - 'assets/**'
      - '_for_authors/**'
      - '_module_templates/**'

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:

  run_script:
    name: Create module_data.py
    runs-on: ubuntu-latest
    # Map a step output to a job output
    outputs:
      changes: ${{ steps.check_if_changed.outputs.changes }}
    steps:
      - uses: actions/checkout@v3
      # Process all the metadata using the script for that
      - name: Process Metadata
        run: |
          bash .github/scripts/process_metadata.sh
      
      # Check if processing the data has changed the python file at all

      - name: Check if anything changed
        id: check_if_changed 
        run: |
          wget https://raw.githubusercontent.com/arcus/education_modules/metadata_workflow/assets/metadata/module_data.py -O current_data_file.txt
          wc current_data_file.txt
          changes=$(diff module_data.py current_data_file.txt | wc -l)
          echo  there is $changes new line
          changes=$([ $changes -gt 0 ] && echo "true" || echo "false")
          echo the changes variable is now $changes
          echo "changes=$changes" >> "$GITHUB_OUTPUT"
      - name: Upload python file as an artifact
        uses: actions/upload-artifact@v3
        with:
          name: metadata_artifact
          path: module_data.py
  # debugging:
  #   runs-on: ubuntu-latest
  #   needs: run_script
  #   steps:
  #     - env:
  #         OUTPUT3: ${{needs.run_script.outputs.changes}}
  #       run: echo "The output of the previous step was $OUTPUT3"

  # This message is for me to check that things ran correctly even if nothing actually updated
  nothing_to_update_message:
    name: No changes to metadata
    runs-on: ubuntu-latest
    needs: run_script
    if: needs.run_script.outputs.changes != 'true'  
    steps:
      - env:
          OUTPUT3: ${{needs.run_script.outputs.changes}}
        name: Nothing was committed
        run: echo "Nothing was committed because the any_change variable was $OUTPUT3"

  # This message is for me to check that things ran correctly even if nothing actually updated
  metadata_changed_message:
    name: Metadata has changed
    runs-on: ubuntu-latest
    needs: run_script
    if: needs.run_script.outputs.changes == 'true'  
    steps:
      - env:
          OUTPUT3: ${{needs.run_script.outputs.changes}}
        name: Metadata has changed
        run: echo "The any_change variable was $OUTPUT3 so now the changes need to be propagated"
  
  # If changes were made to the python file, then we want to push those changes to the metadata_workflow branch and make a new csv
  create_csv:
    name: Create module_data.csv
    runs-on: ubuntu-latest
    needs: metadata_changed_message 
    steps:
      - env:
          OUTPUT3: ${{needs.run_script.outputs.changes}}
        run: echo "The changes variable is $OUTPUT3"
      
      - uses: actions/checkout@v3
      
      - name: Download artifact
        uses: actions/download-artifact@v3
        with:
          name: metadata_artifact

      - name: Check that artifact downloaded
        run: |
          wc module_data.py

      - name: Set up Python 3.x
        uses: actions/setup-python@v4
        with:
          # Semantic version range syntax or exact version of a Python version
          python-version: '3.x'

      - name: Install Python dependencies
        run: python -m pip install --upgrade pip pandas

      - name: Create csv from python file
        run: |
          cp module_data.py create_csv.py
          echo "df.to_csv('module_data.csv')" >> create_csv.py
          python create_csv.py
      
      - name: Upload csv file as an artifact
        uses: actions/upload-artifact@v3
        with:
          name: metadata_csv
          path: module_data.csv

  update_metadata_workflow_branch:
    name: Update metadata_workflow branch
    runs-on: ubuntu-latest
    needs: metadata_changed_message 
    steps:
      - env:
          OUTPUT3: ${{needs.run_script.outputs.changes}}
        run: echo "The changes variable is $OUTPUT3"
      
      - uses: actions/checkout@v3
      
      - name: Download artifact
        uses: actions/download-artifact@v3
        with:
          name: metadata_artifact

      - name: Check that artifact downloaded
        run: |
          wc module_data.py

      - name: Set up Python 3.x
        uses: actions/setup-python@v4
        with:
          # Semantic version range syntax or exact version of a Python version
          python-version: '3.x'

      - name: Install Python dependencies
        run: python -m pip install --upgrade pip pandas

      - name: Create csv from python file
        run: |
          cp module_data.py create_csv.py
          echo "df.to_csv('module_data.csv')" >> create_csv.py
          python create_csv.py

      - name: Commit newly updated files
        run: |
          git config --local user.name actions-user

          git config --local user.email "actions@github.com" 
 
          git fetch

          git checkout atomize_metadata_workflows #CHANGE BACK TO METADATA_WORKFLOW BEFORE MERGING

          mv -f module_data.py module_data.csv assets/metadata/
          
          git add assets/metadata/module_data.py

          git add assets/metadata/module_data.csv

          git commit -am "update metadata records" 
 
          git push origin atomize_metadata_workflows #CHANGE BACK TO METADATA_WORKFLOW BEFORE MERGING

  # The module_discovery repository automatically checks weekly to see if the module data has changed, but a message here informs users how to manually run that workflow.
  update_module_discovery_repository:
    name: Optional--Manually update module_discovery repository
    runs-on: ubuntu-latest
    needs: metadata_changed_message 
    steps:
      - env:
          OUTPUT3: ${{needs.run_script.outputs.changes}}
        run: echo "The changes variable is $OUTPUT3"
      
      - name: Print module_discovery instructions
        run: |
          echo "The module_discovery repository will automatically update on Sunday morning. If you have changed metadata that needs to be updated there immediately, please manually activate that workflow at https://github.com/arcus/module_discovery/actions/workflows/update_module_data.yml"


  # It is possible for the metadata to change without any of the things displayed on the public facing website actually needing to be updated.
  check_list_of_modules_for_changes:
    name: TODO Check the public list for changes
    runs-on: ubuntu-latest
    needs: metadata_changed_message
    steps:
      - name: Check if data on public site has changed
        run: |
          echo "Check if data on public site has changed"

  # The public facing website also has to be updated. 
  update_gh-pages:
    name: TODO Update list of modules on public site
    runs-on: ubuntu-latest
    needs: check_list_of_modules_for_changes
    steps:
      - name: Push changes to public website
        run: |
          echo "Push changes to public website"

